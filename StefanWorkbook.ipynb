{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment Score Prediction\n",
    "## Stefan Keselj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this file is to extract comment features from the Kaggle May 2015 Data and then use non-score features to predict score. Note that the features described in the previous sentence are intermediate features, like a specific comment represented as a string, which will then be further processed into finer features like a bag-of-words vector of that comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "import enchant\n",
    "english_dict = enchant.Dict(\"en_US\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from HTMLParser import HTMLParser\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pylab\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (20, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load my data (andrew's data preprocessed for this task)\n",
    "dftrain = pd.read_csv('/data/train_data_stef.csv',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>There are a lot of small tournaments in CS:GO ...</td>\n",
       "      <td>21</td>\n",
       "      <td>GlobalOffensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I actually managed the Chilkoot trail with a 4...</td>\n",
       "      <td>1</td>\n",
       "      <td>pics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Bruh</td>\n",
       "      <td>1</td>\n",
       "      <td>nba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Here you go](http://ftve3100-i.akamaihd.net/h...</td>\n",
       "      <td>1</td>\n",
       "      <td>nba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Retailers will just jack up the prices across ...</td>\n",
       "      <td>0</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Kobe 392 attempts this year and Jason Williams...</td>\n",
       "      <td>1</td>\n",
       "      <td>nba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>China also invented the e-cig and many other i...</td>\n",
       "      <td>6</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Vampire hunter</td>\n",
       "      <td>1</td>\n",
       "      <td>pics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>[**Foreigners who want to Understand**](http:/...</td>\n",
       "      <td>926</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>&amp;gt; OW is flawed,\\n no its not, if it has les...</td>\n",
       "      <td>0</td>\n",
       "      <td>GlobalOffensive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                                               body score  \\\n",
       "0          0  There are a lot of small tournaments in CS:GO ...    21   \n",
       "1          1  I actually managed the Chilkoot trail with a 4...     1   \n",
       "2          2                                               Bruh     1   \n",
       "3          3  [Here you go](http://ftve3100-i.akamaihd.net/h...     1   \n",
       "4          4  Retailers will just jack up the prices across ...     0   \n",
       "5          5  Kobe 392 attempts this year and Jason Williams...     1   \n",
       "6          6  China also invented the e-cig and many other i...     6   \n",
       "7          7                                    Vampire hunter      1   \n",
       "8          8  [**Foreigners who want to Understand**](http:/...   926   \n",
       "9          9  &gt; OW is flawed,\\n no its not, if it has les...     0   \n",
       "\n",
       "         subreddit  \n",
       "0  GlobalOffensive  \n",
       "1             pics  \n",
       "2              nba  \n",
       "3              nba  \n",
       "4        worldnews  \n",
       "5              nba  \n",
       "6        worldnews  \n",
       "7             pics  \n",
       "8        worldnews  \n",
       "9  GlobalOffensive  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove non-english words, stop-words, punctuation\n",
    "def clean_comment(sentence):\n",
    "    sentence = sentence.decode('utf-8')\n",
    "    parser = HTMLParser()\n",
    "    sentence = parser.unescape(sentence)\n",
    "    # check if a string is a number \n",
    "    def is_number(s):\n",
    "        try:\n",
    "            float(s)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    # check if a string is only ascii characters\n",
    "    def is_ascii(s):\n",
    "        try:\n",
    "            s.decode('ascii')\n",
    "            return True\n",
    "        except UnicodeDecodeError:\n",
    "            print s\n",
    "            return False\n",
    "    # main logic\n",
    "    try:\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens_no_punct = tokenizer.tokenize(sentence)\n",
    "        meaningful_words = [word.lower() for word in tokens_no_punct \n",
    "                            if not is_number(word)\n",
    "                            and word.lower() not in stopwords.words('english') \n",
    "                            and english_dict.check(word.lower())\n",
    "                            and is_ascii(word)]\n",
    "        return (\" \".join( meaningful_words ))\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size = 1000\n",
    "clean_train_comments = []\n",
    "for i in xrange(0, size):\n",
    "    clean_train_comments.append(clean_comment(dftrain['body'][i]))\n",
    "train_data_features = vectorizer.fit_transform(clean_train_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_features_array = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[**Foreigners who want to Understand**](http://imgur.com/gallery/6NfmQ)\n",
      "\n",
      "I wrote a rather melodramatic post earlier explaining the situation surrounding Lulaâ€™s nomination as Rousseff's Chief of Staff and now its developments. Re\n",
      "[ 0  1  1  1  1  0  0  0  0  1  2  0  0  1  0  0  0  0  1  0  0  0  2  0  0\n",
      "  0  0  1  0  0  1  5  0  1  1  0  1  1  0  0  0  0  0  2  1  0  0  1  2  1\n",
      "  0  0  2  0  0  0  3  2  1  2  0  0  0  2  0  0  1  0  0  0  1  1  1  0  0\n",
      "  1  0  1  0  1  2  0  0  0  0  0  0  1  1  0  0  0  0  1  0  0  1  0  0  1\n",
      "  1  1  0  0  1  1  0  1  0  0  0  0  1  1  1  0  7  0  0  0  2  2  1  2  2\n",
      "  1  0  1  0  2  1  0  0  0  4  1  1  0  0  0  0  0  1  0  0  5  1  2  0  0\n",
      "  0  0  1  0  0  2  3  0  1  0  1  3  1  1  0  2  0  2  0  4  0  0  2  1  0\n",
      "  0  0  0  3  2  1  0  0  0  0  0  1  0  1  0  0  0  2  1  1  1  0  1  0  0\n",
      "  0  1  1  0  1  1  1  0  0  0  1  0  2  0  1  0  0  1  0  0  0  1  0  0  1\n",
      "  0  1  0  0  2  1  0  0  1  0  0  0  0  1  1  0  2  0  2  0  0  0  0  0  1\n",
      "  1  0  1  4  0  0  0  1  1  0  1  2  0  0  1  0  0  0  0  1  0  9  0  0  1\n",
      "  1  0  0  0  1  1  1  0  0  1  1  0  1  1  0  0  0  3  0  1  0  4  0  1  0\n",
      "  0  0  0  1  0  1  3  1  0  0  0  0  0  0  0  0  0  4  0  0  1  0  0  3  1\n",
      "  1  0  0  0  2 10  1  1  0  1  0  0  0  2  0  0  1  0  0  2  0  0  0  1  1\n",
      "  0  0  1  0  4  1  1  0  0  0  1  0  1  0  1  1  2  0  0  0  0  1  0  1  0\n",
      "  1  0  1  1  0  0  2  0  0  9  2  0  0  0  1  0  1  1  0  0  1  1  0  1  1\n",
      "  2  0  0  0  2  0  1  0  1  1  0  3  1  7  0  0  0  1  4  0  1  0  1  1  1\n",
      "  3  0  0  1  1  1  0  0  3  1  1  1  0  0  4  1  0  2  0  0  0  0  0  0  1\n",
      "  0  0  1  2  0  0  0  1  0  0  0  0  0  0  0  1  0  2  1  0  1  0  0  0  0\n",
      "  0  2  1  1  0  0  0  0  0  9  0  1  0  2  2  1  0  3  1  0  2  1  1  0  1\n",
      "  1  2  1  2  2  0  0  0  0  1  3  1  0  2  3  1  0  1  0  0  0  4  0  1  1\n",
      "  0  0  0  0  0  3  1  1  1  0  0  1  0  0  0  0  3  0  0  1  4  0  2  1  0\n",
      "  3  0  1  0  0  1  7  0  0  4  1  0  1  0  2  0  1  2  3  1  1  1  1  0  3\n",
      "  0  0  0  0  1  0  3  0  0  0  0  1  1  2  1  0  0  1  3  0  0  0  0  0  0\n",
      "  0  1  0  1  0  0  0  0  0  1  0  8  1  4  1  0  1  0  1  1  1  1  0  0  1\n",
      "  2  0  0  0  2  0  0  1  1 10  1  0  0  0  1  2  0  0  1  1  0  1  0  0  0\n",
      "  1  1  2  2  1  6  1  1  0  2  0  1  0  1  0  1  0  0  1  0  0  2  0  0  1\n",
      "  1  1  0  0  0  1  0  3  1  1  0  0  1  0  1  0  3  0  0  0  1  1  1  0  0\n",
      "  1  1  1  0  1  1  0  4  0  0  1  0  0  0  0  0  3  0  2  1  1  1  0  1  1\n",
      "  0  0  0  0  0  1  0  1  0  0  0  1  2  0  1  2  0  0  3  0  1  0  0  1  1\n",
      "  2  0  0  1  0  0  0  0  1  1  1  0  1  1  1  3  0  2  0  0  1  0  2  1  1\n",
      "  0  0  0  1  0  0  0  0  0  0  1  0  0  3  0  0  1  0  0  0  7  0  1  0  0\n",
      "  0  0  4  1  0  0  0  1  0  0  0  0  1  0  0  0  0  1  3  1  0  0  0  0  0\n",
      "  0  0  1  2  0  1  4  0  1  0  1  0  0  1  3  0  3  3  0  1  0  1  0  1  0\n",
      "  0  1  2  1  1  0  0  0  1  0  1  1  0  0  2  1  0  3  0  1  0  2  1  4  1\n",
      "  2  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  2  2  1  0  3\n",
      "  1  1  0  0  0  4  0  0  0  1  1  0  0  0  1  0  0  0  0  0  1  1  0  0  1\n",
      "  1  1  0  2  0  0  0  0  1  1  1  0  1  0  1  0  0  0  1  0  0  2  0  1  1\n",
      "  4  0  1  2  0  0  0  1  1  1  0  0]\n"
     ]
    }
   ],
   "source": [
    "print dftrain['body'][8][0:230]\n",
    "print train_data_features_array[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Word to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Skip-thought vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 2grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Sketchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clean = []\n",
    "#for i in xrange(0,100):\n",
    "#    print i\n",
    "#    print clean_comment(dftrain.iloc[i]['body'])\n",
    "#    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for i in range(0,10):\n",
    "#    print i\n",
    "#    print df_pic.iloc[i]['score']\n",
    "#    print df_pic.iloc[i]['body']\n",
    "#    print str(clean_comment(df_pic.iloc[i]['body']))\n",
    "#    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pic = dftrain[dftrain.subreddit==\"pics\"]\n",
    "df_wne = dftrain[dftrain.subreddit==\"worldnews\"]\n",
    "df_fun = dftrain[dftrain.subreddit==\"funny\"]\n",
    "df_aww = dftrain[dftrain.subreddit==\"aww\"]\n",
    "df_gof = dftrain[dftrain.subreddit==\"aww\"]\n",
    "df_nba = dftrain[dftrain.subreddit==\"nba\"]\n",
    "df_cje = dftrain[dftrain.subreddit==\"circlejerk\"]\n",
    "df_sublist = [df_pic, df_nba, df_wne, df_fun, df_aww, df_gof, df_cje]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
