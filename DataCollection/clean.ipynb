{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import scrapy as sc\n",
    "from html.parser import HTMLParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First step of cleaning is getting rid of stuff you don't need and filtering\n",
    "data = pd.read_csv(\"../data/alldata.csv\")\n",
    "data = data[data.author != 'AutoModerator']\n",
    "data = data[data.body != '[deleted]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&gt; Or at least *we're* willing to pretend that they are.\n",
      "\n",
      "FTFY\n",
      "> Or at least *we're* willing to pretend that they are.\n",
      "\n",
      "FTFY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:4: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n"
     ]
    }
   ],
   "source": [
    "# Second step of cleaning is generating the links for each of the comments and cleaning out special characters\n",
    "parser = HTMLParser()\n",
    "linkArray = []\n",
    "for index, row in clean1data.iterrows():\n",
    "    body = parser.unescape(data.iloc[index]['body'])\n",
    "    subreddit = clean1data.iloc[index]['subreddit']\n",
    "    link_id = re.sub('t[0-9]_', '', clean1data.iloc[index]['link_id'])\n",
    "    comment_id = clean1data.iloc[index]['id']\n",
    "    string = 'http://www.reddit.com/r/' + subreddit + '/comments/' + link_id + '/c/' + comment_id\n",
    "    print(string)\n",
    "    if index % 100 == 0:\n",
    "        print(index)\n",
    "    linkArray.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data = data[data.body != '[deleted]']\n",
    "data = data[['body', 'created_utc', 'subreddit_id', 'link_id', 'parent_id', 'score', 'id', 'subreddit']]\n",
    "data.to_csv('../data/clean1data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean1data = pd.read_csv(\"../data/clean1data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Third step is visiting the links and getting the submission title and (if it's a self-post) body content\n",
    "# class RedditSpider(sc.Spider):\n",
    "#     name = 'reddit'\n",
    "#     start_urls = clean1data[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
